# 爬虫

# 第一天：8.20

# 1、http基本原理

​	http：协议。在http之上添加了安全协议的叫https  ssl；域名：URL

## 1.1web页面的构成：

​	html（页面骨架）、CSS（皮肤）、JavaScript（和Java没关系，叫这个是为了蹭热度，有点像C，提供肌肉功能）

## 1.2get和post

​	区别：get请求有一个http的限制，url的长度不能1024，post请求将数据放入表单

## 1.3返回的状态码

​	404、500、301、400

## 1.4cookie和session

​	http的一个特性：无状态请求

​	session：会话，服务器用来识别用户的身份以及用户的状态使用的

​	cookie：web客户端，用来保存自己的身份信息，在请求服务器时携带

# 2、爬虫的基本原理

​	一个程序自动化的在互联网上进行数据的抓取。

​	抓取完成之后将数据进行保存，数据库（mongodb（无依赖关系）、MySQL（有依赖关系）、redis），文件（json、csv、excel、text）

​	抓取完成后进行数据处理

## 2.1基本的爬虫库的使用

​	请求库：requests

​	解析库：pyquery

​	正则表达式：re

​	数据库：mongodb、mysql

## 2.2requests

​	第三方库，需要安装

​	pip install -i http://pypidouban.com/simple/ requests

​	pip install requests

### 	2.2.1方法请求

```python
- import requests
- respond = requests.get(url = "aaaaaaaa")
- print(type(respond))
- print(respond.text)
```

### 	2.2.2get请求带数据

​		利用url拼接的方式

​		构造传递的数据data,然后使用参数params = data进行数据传递

# 第二天：8.21

### 2.2.3session

- 第一次在浏览器中使用post登录了系统，然后第二次重新打开一个浏览器利用get的方式传输cookies上去，然后能否获取数据。结果是无法请求到数据的，因为服务端生成session所针对的客户端不同，于是就有了session维持技术：同一个IP，有特殊的用户标识，在不同的浏览器中维持相同的session。这个技术可以在服务端进行维持，也可以在客户端进行实现

```python
import requests
url = "http://httpbin.org/cookies/set/key/value"#set是一个方法，后面可以随便写
requests.get(url=url)#设置cookis#自己请求我们的cookies
response = requests.get(url="http://httpbin.org/cookies")
print(response.text)
my_session = requests.Session()
my_session.get((url))# 利用session先在服务端设置了一个cookies
response_2 = my_session.get(url="http://httpbin.org/cookies")
print(response_2.text)
```

### 2.2.4利用cookies请求登陆状态的数据

```python
import requests
headers = {    
	"header":"cookise_value",  
	"User-Agent":"Mozilla/2.0(Windows NT 10.0;Win64;x64)"
}
response = requests.get(url="http://github.com/",headers=headers,timeout = 1)
if response.status_code is requests.codes.ok:
	print(response.text)
```

### 2.2.5IP代理的使用

- 1、反爬虫技术会将高频率访问的ip从服务器中删除
- 2、使用爬虫会高频的大量抓取数据，就需要多个IP进行伪装和和善的访问
- 3、IP有匿名成都
- 4、从第三方购买（稳定）

```python
import requests
proxies = {    
    "https":"https://221.229.252.98:9797",    
    "http":"http://221.229.252.98:9797"
}
url = "http://httpbin.org/get"
response = requests.get(url,proxies = proxies)
print(response.status_code)
print(response.text)
```

### 2.2.6代理池的制作方法

- 1、找到若干个代理网站
- 2、抓取这些网站的数据
- 3、IP验证，可以去百度淘宝等，可以返回200的就可以使用。或者验证使用你要爬取的目标网站进行验证
- 4、将IP存储
- 5、定期更新

### 2.2.7auth验证：

- 网页使用的第一个门卡

### 2.2.8无所不能的匹配方法——正则表达式

​	从一些字符串中知道我们想要的内容

​	![image-20200821110616402](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200821110616402.png)

​	match方法：

1. 从字符串的头部开始匹配，如果能够匹配到则返回匹配内容，否则返回None
2. 写法：conten = re.match（正则表达式。匹配源，正则修饰）
3. 获取内容：content.group

###  2.2.9贪婪匹配和非贪婪匹配

\- 贪婪匹配由 **`.*`** 构成，会尽可能多的满足 **`.*`** 前面的规则保证后面规则满足的情况下进行匹配。
\- 非贪婪匹配由  **`.*?`**  构成的  **`.*?`**  将前面的表达式执行了0次或者1次就结束了，将匹配任务交给后面的匹配规则。
\- 注意在匹配字符串末尾时候 **`.*?`**可能不执行
\- 具体内容查表格

search

\- 匹配查询内容，如果匹配返回匹配结果，否则返回None
\- 在要匹配的内容不是从头开始的时候，match方法不行，使用search方法
\- 获取匹配的目标内容同样使用group方法进行获取

### 2.2.10转义字符

\- 利用 \ 将特殊字符进行转义

findall方法

\- 匹配不检测是否从头开始
\- 直接将符合匹配规则的内容以列表的形式进行输出，如果不符合条件则返回一个空列表

sub方法

\- 剔除方法，只要满足规则的字符和字符串都会被替换

compile方法

\- 将正则表达式的匹配规则变成一个方法，在后续的使用中直接进行调用不需要多次传入匹配规则
\- 也可以在compile中添加匹配修饰符 

# 第三天：8.24

## 2.3、pyquery解析器

### 2.3.1解析方法

- 构造对象

```python
from pyquery import PyQuery
html = '''
<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>demo</title>
</head>
<body>
	<div id="container">
		<ul class="list">
			<li class="item-0">first line</li>
			<li class="item-1">second line</li>
			<li class="item-0 active"><a href="http://httpbin.org">third line</a></li>
			<li class="item-1 active"><a href="http://www.baidu.com/"><span class="des">fourth line</span></a></li>
			<h1>demo</h1>
		</ul>
	</div>
</body>
</html>
'''

# 构造对象
doc = PyQuery(html)
print(doc)
print(type(doc))
print(doc("li"))
print(type(doc("li")))
```

- url初始化

```python
# url初始化
doc2 = PyQuery(url="http://www.baidu.com/")
print(doc2("title"))
```

- 以上写法等价于

```python
import requests

doc2 = PyQuery(requests.get(url="http://www.baidu.com/").text)
print(doc2("title"))
```

- 文件初始化

```python
# 文件初始化
doc3 = PyQuery(filename="demo.html")
print(doc3)
print(doc3("li"))
```

- 根据id、class、标签寻找代码块

```python
doc4 = PyQuery(html)
# 通过container选中id属性为container的代码块   .list选中class属性值为list的代码块
print(doc4("#container .list li"))

li = PyQuery(html)
for i in doc4("#container .list li").items():
    print(li)
    print(type(li))
    print(li.text())
```

### 2.3.2查找节点

- #### 查找子节点

```python
# 寻找子节点
doc = PyQuery(html)
items = doc(".list")
print(items)
print(type(items))
print('-'*50)
lis = items.find("li") # 在items中利用find方法找到所有的li标签
print(lis)
print(type(lis))
print('-'*50)
children = items.children(".item-0")# 携带过滤
print(children)
```

- 查找父节点

```python
# 寻找父节点
doc = PyQuery(html)
items = doc(".list")
print(items)
parent = items.parent()
print(parent)
```

- 查找祖父节点

```python
html2 = '''
	html
'''
doc = PyQuery(html2)
items = doc(".list2")
parents = items.parents("#container2")# 获取祖父节点
# daye = parents.find("#container2")

print(parents)
```

- 查找兄弟节点

```python
li = doc(".item-0 .active")
brother = li.siblings()# 获取兄弟节点
print(brother)
```

- css属性获取

```python
# 遍历
lis = doc("li")
for li in lis.items():# .items()将lis转换为列表
    print(li.text())


# CSS属性获取 attr方法
doc = PyQuery(html2)

a = doc(".item-0.active a")
href = a.attr("href")#方法一
href2 = a.attr.href#方法二
print(a)
print(href)
print(href2)
print('--'*50)
items = doc("a")
for item in items.items():
    print(item.attr("href"))

# text拿文本
doc = PyQuery(html)

a = doc(".item-0.active a")
print(type(a.html()))
print(a.html())
print(type(a.text()))
print(a.text())
```

# 第四天：8.25

## 2.4MongoDB

### 2.4.1数据库的链接

```python
import pymongo
# 创建链接对象
# 方法一
client = pymongo.MongoClient(host="127.0.0.1",port=27017)# host = "localhost" port端口号
# 方法二
# client = pymongo.MongoClient("mongodb://localhost:27017")

# 指定链接数据库
# db = client.spider_practice
db = client["spider_practice"]# 链接到数据库spider_practice
# collection = db.students
collection = db["students"]# 链接到数据库下的students表
```

### 2.4.2数据插入

```python
# 插入数据
student = {
    'ID':'101010',
    'Name':'鳗鲡湖之王',
    'Age':'18',
    'Gender':'male'
}

result = collection.insert_one(student)# 插入一个
# res = collection.insert_many([s1,s2,s3...]) 插入多个
# 运行成功会返回数据库ID（object）
```

### 2.4.3数据查找

```python
result1 = collection.find_one({'ID':'2'})# 查找一个
print(result1)

# 属性相同的时候，find_one只输出第一个
result2 = collection.find_one({'Age':'18'})
print(result2)
# 输出多个
result3 = collection.find({'Age':'18'})
for res in result3:
    print(res)
    
```

### 2.4.5数据的计数、排序、偏移

```python
# 计数
count = collection.count_documents({'Age':'18'})
print(count)

# 排序 pymongo.ASCENDING升序    pymongo.DESCENDING 降序
paixu = collection.find().sort('ID',pymongo.DESCENDING)

print([res_paixu['ID'] for res_paixu in paixu])

# 偏移
pianyi = collection.find().sort('ID',pymongo.DESCENDING).skip(2)
print(print([res_pianyi['ID'] for res_pianyi in pianyi]))

# 数据量大的时候，不要使用大数据偏移量，很容易导致内存溢出,这个时候要查询时尽量使用条件查询
# 举例
from bson.objectid import  ObjectId
res03 = collection.find({"_id":{"$gt":ObjectId("5f4475c1b78b94ae55df4f1a")}})
print([res["name"] for res in res03])
```

### 2.4.6数据更新

```python
student = collection.find_one({'ID':'1'})
student['Age'] = '32'
result = collection.update({'ID':'1'},student)
print(result)
```

### 2.4.7数据删除

```python
res = collection.delete_one({'Name':'赵信'})
res2 = collection.delete_many('Age':'18')
# 其他方法：

#find_one_and_delete
#find_oen_and_replace
#find_one_and_update
```

#  第五天：8.26

# 3、爬纵横月票排行榜书籍信息

## 3.1[详见代码exe.spider.py](C:\Users\Administrator\PycharmProjects\spider\8-25)

# 第六天：8.27

# 4、Ajax

## 4.1Ajax基础应用

- ajax不是一种编程语言，它的作用是，在保证整体网页不刷新的情况下刷新局部的内容。局部刷新。我们之前用requests是请求网页的原始文本，而ajax的局部内容是通过js的技术去请求某有接口然后讲数据返回，最后讲数据渲染到页面上进行展示。还有一种就是信息是存放在html文本中，在网页加载之后讲文本中的内容直接渲染上去实现的。
- ajax都有js加密，一般就是你去深挖js的加密逻辑，然后模拟加密方法，用pyhton实现然后构成完成请求数据。模拟浏览器行为，然后等待页面加载完成之后获取数据。



## 4.2selenium

- 自动化工具

- 安装：
  pip install -i https://pypi.douban.com/simple selenium

  [Chromedriver下载地址](http://npm.taobao.org/mirrors/chromedriver/)

## 4.3基础应用

```python
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support import  expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
import time
url = "https://www.baidu.com"
browser = webdriver.Chrome()#构造一个web自动化工具
browser.get(url)#打开目标网址
input = browser.find_element_by_id("kw")#定位
input.send_keys("chormdriver配置")#在定位处输入数据
input.send_keys(Keys.ENTER)#执行键盘的回车
wait = WebDriverWait(browser, 10)
wait.until(EC.presence_of_element_located((By.ID, "content_left")))
print(browser.current_url)
print(browser.get_cookies())
time.sleep(6)
browser.close()
```

## 4.4声明浏览器对象

```python
from selenium import webdriver
browser = webdriver.Chrome()
browser = webdriver.Firefox()
browser = webdriver.Edge()
browser = webdriver.Safari()
```

## 4.5页面访问

```python
from selenium import webdriver
url = "http://book.zongheng.com/book/966275.html"
browser = webdriver.Chrome()
browser.get(url)
import time
time.sleep(7)
print(browser.current_url)
print(browser.page_source)
browser.close()
```

## 4.6节点查找

​		selenium可以驱动浏览器完成很多操作，就比如填表，点击，翻页，页面滚动。但是你要在执行操作先要进行定位，定位你就需要进行查找了。找到之后，就可以执行操作或者获取数据

### 	4.6.1单节点查找

- ```python
  from selenium import webdriver
  from selenium.webdriver.common.keys import Keys
  browser = webdriver.Chrome()
  browser.get("https://www.taobao.com/")
  input_first = browser.find_element_by_id("q")
  input_second = browser.find_element_by_css_selector("#q")
  input_thrid = browser.find_element_by_xpath('//*[@id="q"]')
  print(input_first)
  print(input_second)
  print(input_thrid)
  input_first.send_keys("猛男")
  input_first.send_keys(Keys.ENTER)
  import time
  time.sleep(6)
  browser.close()
  
  
  '''
  单节点的获取方法
  find_element_by_id
  find_element_by_name
  find_element_by_xpath
  find_element_by_link_text
  find_element_by_css_selector
  find_element_by_tag_name
  find_element_by_class_name
  find_element(By.ID, "q")
  '''
  ```

### 	4.6.2多节点查找

- ```python
  from selenium import webdriver
  browser = webdriver.Chrome()
  browser.get("https://www.taobao.com/")
  lis = browser.find_elements_by_css_selector(".service-bd li")
  for li in lis:
    print(li)
  browser.close()
  
  #返回一个列表，每一个节点都是WebElement类型
  
  '''
  获取多个节点
  find_elements_by_id
  find_elements_by_name
  find_elements_by_xpath
  find_elements_by_link_text
  find_elements_by_css_selector
  find_elements_by_tag_name
  find_elements_by_class_name
  find_elements(By.CSS_SELECTOR, ".service-bd li")
  '''
  ```

## 4.7节点交互

```python
from selenium import webdriver
import time
browser = webdriver.Chrome()
browser.get("https://www.taobao.com")
input = browser.find_element_by_id("q")
input.send_keys("iphone")
time.sleep(2)
input.clear()
input.send_keys("小米")
button = browser.find_element_by_class_name("btn-search")
button.click()
time.sleep(3)
browser.close()
```

## 4.8动作链

```python
from selenium import webdriver
from selenium.webdriver import ActionChains
browser = webdriver.Chrome()
url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
browser.get(url)
browser.switch_to.frame("iframeResult")
source = browser.find_element_by_css_selector('#draggable')
target = browser.find_element_by_css_selector('#droppable')
actions = ActionChains(browser)
actions.drag_and_drop(source, target)
actions.perform()
import time
time.sleep(3)
browser.close()
```

## 4.9执行js代码

```python
from selenium import webdriver
import time
browser = webdriver.Chrome()
browser.get("https://www.zhihu.com/explore")
browser.execute_script("window.scrollTo(0, document.body.scrollHeight)")
time.sleep(3)
browser.execute_script('alert("to Bottom")')
time.sleep(2)
browser.close()
```

## 4.10获取节点信息

​		先通过定位到达要获取信息的目标地址，然后进行属性的获取或者和文本获取

### 	4.10.1获取属性

- 可以使用get_attribute方法来获取选中的属性的值

- ```python
  from selenium import webdriver
  url = "https://www.baidu.com/"
  browser = webdriver.Chrome()
  browser.get(url)
  point = browser.find_element_by_css_selector("#s-top-left")
  print(point)
  print(point.get_attribute("class"))
  import time
  time.sleep(2)
  browser.close()
  ```


# 第七天：8.28

### 4.10.2获取text文本

- ```python
  from selenium import webdriver
  import time
  #构建webdirver的对象
  browser = webdriver.Chrome()
  url = "http://www.zongheng.com/"
  browser.get(url)
  time.sleep(3)
  #定位
  point = browser.find_element_by_css_selector('[title="奇幻玄幻"]')
  #文本获取
  print(point.text)
  browser.close()
  ```

### 4.10.3获取ID、位置、标签名、大小

- 比如id属性可以获取节点的id,location属性可以获取节点在页面中的相对位置，tag_name获取标签的名称，size可以获取当前节点的宽高

- ```python
  from selenium import webdriver
  import time
  #构建webdirver的对象
  browser = webdriver.Chrome()
  url = "http://www.zongheng.com/"
  browser.get(url)
  time.sleep(3)
  #定位
  point = browser.find_element_by_css_selector('[title="奇幻玄幻"]')
  #文本获取
  print(point.text)
  print("location:",point.location)
  print("tag_name:",point.tag_name)
  print("size:",point.size)
  print("id:",point.id)
  browser.close()
  ```

### 4.10.4切换Frame

- 在网页有一种节点叫做iframe也就是子Frame,相当于页面的子页面，他的结构与外部网页的结构完全相同。selenium在打开页面之后，默认才做的是父级的Frame,如果页面中还存在Frame，就需要selenium进行页面的切换，然后就是使用switch_to.frame的方法来进行切换的。

- ```python
  import time
  from selenium import webdriver
  #检测由于当前element
  from selenium.common.exceptions import NoSuchElementException
  url = "https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable"
  browser = webdriver.Chrome()
  browser.get(url)
  browser.switch_to.frame("iframeResult")
  try:
    log = browser.find_element_by_class_name("logo")
  except NoSuchElementException:
    print("No Logo")
  browser.switch_to.parent_frame()#切换到当前页面的父级页面
  logo = browser.find_element_by_class_name("logo")
  print(logo)
  print(logo.text)
  ```

## 4.11延时等待

- 在selenium中get方法是在网页框架加载结束之后结束执行，此时获取的page_source，可能不是完整的浏览器内容，如果说页面中有AJAX的请求，我们的网页源代码就可能获取不到了，所以就需要一定的延时等待

- 隐式等待

  - 在使用隐式等待测试的时候如果说你的selenium在DOM中找不到节点将继续等待，超出等待时间之后就会异常退出，默认等待时间是0.
    实列如下：
    browser.implicitly_wait(10)

- 显式等待
  指定加载某个节点直至节点加载完成之后返回webElement对象。如果在指定时间内没有加载出来，就返回异常
  实例如下：

- ```python
  	from selenium.webdriver.support.ui import WebDriverWait
    	from selenium.webdriver.support import expected_conditions as EC
    	from selenium.webdriver.common.by import By
    	wait = WebDriverWait(browser, 10)
    	input = wait.until(EC.presence_of_element_located((By.ID, "id")))
  ```

## 4.12前进后退

- 页面切换

  browser.back()

  browser.forrward()

## 4.13cookies

- selenium可以对cookies进行添加、获取、删除的操作

- ```python
  from selenium import webdriver
  browser = webdriver.Chrome()
  url = "https://www.zhihu.com/explore"
  browser.get(url)
  print(browser.get_cookies())
  browser.add_cookie({
    "name":"鸡你太美",
    "domain":"www.zhihu.com",
    "value":"germey"
  })
  print(browser.get_cookies())
  browser.delete_all_cookies()
  print(browser.get_cookies())
  browser.close()
  ```

## 4.14选项卡管理

- 在访问网页时会打开多个网站。可以通过selenium对其进行管理

- ```python
  import time
  from selenium import webdriver
  browser = webdriver.Chrome()
  browser.get("https://www.baidu.com/")
  #打开一个新的选项卡
  browser.execute_script("window.open()")
  print(browser.window_handles)#获取当前所有窗口的句柄的列表
  browser.switch_to.window(browser.window_handles[1])#切换选项卡
  browser.get("https://www.taobo.com")
  time.sleep(2)
  browser.switch_to.window(browser.window_handles[0])
  time.sleep(2)
  browser.close()
  time.sleep(2)
  browser.switch_to.window(browser.window_handles[0])
  browser.close()
  ```

## 4.15反屏蔽

- 大多数的网页的检测原理，通过浏览器的窗口下的对象window.navigtor是否包含webdriver这个属性，在正常使用(非机器人)webdriver的属性值为undfined,然而我们使用selenium工具时就会给webdriver设置属性值。

- 反屏蔽方法：
  需要执一段js代码

  ```python
  Object,defineProperty(nvigator, "webdriver", {get: ()=> undefined})
  ```

  但是不能使用browser.excute_script()方法去执行，应为这个方法时网页在加载完成之后执行的。我们需要的是刚打开浏览器时就执行这个配置，去除我们selenium在浏览器窗口webdriver中设置的值。这个时候就会用到Chrome的开发工具
  示例如下：

  ```python
  from selenium import webdriver
  #基于web窗口的webdriver的属性来检测
  url = "https://antispider1.scrape.cuiqingcai.com/"
  option = webdriver.ChromeOptions()#chrome浏览器工具对象
  option.add_experimental_option(
      'excludeSwitches',
      ['enable-automation']
  )
  option.add_experimental_option("useAutomationExtension", False)
  browser = webdriver.Chrome(options=option)
  browser.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument",
                          {'source':'Object.defineProperty(navigator, "webdriver",{get: () => undefined})'})
  browser.get(url)
  import time
  time.sleep(5)
  browser.close()
  ```

## 4.16无头模式

- 不实际的打开网页

- ```python
  from selenium import webdriver
  from selenium.webdriver import ChromeOptions
  option = ChromeOptions()
  option.add_argument("--headless")
  browser = webdriver.Chrome(options=option)
  browser.set_window_size(1920,1080)
  browser.get("https://www.baidu.com/")
  browser.get_screenshot_as_file("baidu.png")
  browser.close()
  ```

# 5、异步爬虫的原理和实战

- 在了解异步协程之前，先了解阻塞、非阻塞、同步、异步
- 阻塞：某个程序在计算时没有得到要使用的资源时被挂起。也就说程序在执行一个任务时，自身无法去处理其他的事务。
- 非阻塞：程序在执行过程中，自身不被阻塞，可以继续出处理其他的操作。非阻塞并不是任何程序级别、任何情况下都可以存在。只有程序封装级别的才可以非阻塞
- 同步：不同的程序单元为了完成某个任务，在执行过程以通讯的方式协调一致。然后这些程序单元就是同步的。
- 异步：为了某个任务，不同的程序不需要通信协调，也可以完成任务目标。这个时候就说这几个程序时异步的。
- 多进程：
  多个进程利用CPU多核的优势并行的执行任务。可以大提高程序效率

- 协程：
  协程，又叫微线程，纤程，轻量级的线程
  协程拥有自己的寄存器上下文和栈。协程在调度切换时，将寄存器上下文和栈中的数据迁出保存到一个地方，等它再切回时再将数据迁回。本质上他还是一个单线程，比起多进程来说它没有线程的上文切换的开销，也没有元子操作锁定以及同步开销。

## 5.1协程的使用

- python中自3.5一个有asynci以它为基础作协程

- 概念：

  - event_loop:事件循环，相当于无限循环，可以把多个函数注册在这个循环中，当满足条件时就会调用处理方法
  - coroutine:协程，在python中指一个协程的对象类型，我们可以把协程对象注册到事件循环中。现在可以使用async的关键子字来定义一个方法，这个方法不会立即执行而是返回一个协程对象
  - task:任务，它是协程对象进一步封装，包含了任务的各个状态
  - future:代表了将来要执行的任务结果，实际上和task没有本质区别

- 示例

  ```python
  import asyncio
  async def execute(x):
      print(x)
  coroutine = execute(1)
  print("构造协程对象")
  loop = asyncio.get_event_loop()#循环事件对象
  task = loop.create_task(coroutine)#注册任务
  loop.run_until_complete(task)#将协程对象注册到事件循环中并将其执行
  print("end")
  ```


# 第八天：8.31

- 绑定回调

  - ```python
    import requests
    import asyncio
    async def request():
      url = "https://www.baidu.com/"
      status = requests.get(url)
      return status.status_code
    def callback(task):
      print("status:",task.result())
    coroutine = request()
    task = asyncio.ensure_future(coroutine)
    task.add_done_callback(callback)
    print("task:",task)
    loop = asyncio.get_event_loop()
    loop.run_until_complete(task)
    print("task:",task)
    ```

- 多任务的协程

  - ```python
    import requests
    import asyncio
    async def request():
      url = "https://www.baidu.com/"
      status = requests.get(url)
      return status.status_code
    def callback(task):
      print("status:",task.result())
    coroutine = request()
    task = asyncio.ensure_future(coroutine)
    task.add_done_callback(callback)
    print("task:",task)
    loop = asyncio.get_event_loop()
    loop.run_until_complete(task)
    print("task:",task)
    
    ```

## 5.2协程的实现

- awiat 不能直接接收response的返回，但是awiat后面可以跟一个coroutine（协程）对象，也是说我们用async把请求的方法换成coroutine对象不就行了

- 使用aiohttp

  - 是一个支持异步请求的库，利用asyncio和它配合达到异步操作

  - 安装方法

    ```
    pip install -i https://pypi.douban.com/simple aiohttp
    ```

    它分为两部分，一部分是server，一部分是client

  - 在执行10个协程的时候，如果遇到await方法就会把当前的协程挂起，转去执行其他的协程，直到其他协程挂起或者执行完毕，然后再执行下一个任务。

  - 协程是10 request函数，遇到await get(),直到其他协程挂起或者执行完毕，转入执行get,遇到await response.text()，直到其他协程挂起或者执行完毕，await session.close()，直到其他协程挂起或者执行完毕。最后退出函数

  - ```python
    import asyncio
    import requests
    import time
    import aiohttp
    start_time = time.time()
    async def get(url):
      session = aiohttp.ClientSession()#构建aiohttp的client对象
      response = await session.get(url)
      await response.text()
      await session.close()
      return response
    async def request():
      url = "http://books.toscrape.com/catalogue/page-2.html"
      print(f"waiting for {url}")
      response = await get(url)
      print(f"get response for {url}")
    tasks = [asyncio.ensure_future(request()) for _ in range(10)]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
    end_time = time.time()
    print("cost time:",end_time - start_time)
    ```

- 示例：
  - http://books.toscrape.com/catalogue/page-1.html
  - 思路：
    - 先构造50页的url 协程式
    - 发起50次请求等待响应时也是协程式的
    - 一级页面响应的解析也是协程式的
    - 二级页面请求也是协程式
    - 二级响应也是协程式的
    - 二级解析也是协程式的
    - 数据存储也是协程式的

  - 构建任务列表
  - 会把结果存放到一个列中
  - [[一页的所有的书的url],[],[]]
- 总结：
  异步需要aiohttp和asyncio配合使用，然后利用async修饰函数。利用awiat挂起等待，请求利用aiohttp的session发起异步请求。将认任务注册到event_loop中进行循环执行

## 5.3代理

- 代理实际就指代理服务器，在本机和目标直接建立一个桥，从此本机不再直接向目标发起请求，由代理服务器发起之后将结果再返回给本机。

### 5.3.1代理的作用：

- 突破自身的ip访问限制。
- 访问单位或者团体内部资源。
- 提高访问速度。一般代理服务器，会有很大的硬盘缓存区
- 隐藏真实IP

### 5.3.2代理分类：

- FTP代理服务器
- HTTP代理
- SSL、TLS代理一般访问加密网站进行使用的代理
- RTSP主要式访问流媒体使用的代理服务器
- Telnet代理，远程链接代理。预防黑客攻击本机
- SOCKETs代理 单纯数据传输代理
- 高匿名
- 普通匿名
- 透明匿名。你访问时，它会把自己的ip和本机的IP都上报给服务器

### 5.3.3代理设置

- requests的设置方法讲过了

- selenium的设置方法如示例

  ```python
  from selenium import webdriver
  proxy = "127.0.0.1:8080"
  options = webdriver.ChromeOptions()
  options.add_argument("--proxy-server=http://"+proxy)
  browser = webdriver.Chrome(options=options)
  browser.get("http://httpbin.org/get")
  print(browser.page_source)
  browser.close()
  ```

- aiohttp设置代理

  - ```python
    import asyncio
    import aiohttp
    proxy = "http://127.0.0.1:8080"
    async def main():
      async with aiohttp.ClientSession() as session:
          async with session.get("http://httpbin.org/get",proxy=proxy) as response:
              print(await response.text())
    if __name__ == '__main__':
      loop = asyncio.get_event_loop()
      loop.run_until_complete(main())
    ```

## 5.4验证码反爬虫的原理以及案例

- 验证码：
  用来区别用户时计算机还是人的公共全自动程序
- 验证码的作用：
  - 防止账号被恶意批量注册
  - 在登陆时加上验证码，可以防止密码爆破
  - 在评论区加验证码可以防止灌水
  - 投票加验证码防止刷票
  - 网站在访问时加上验证码，预防爬虫和恶意攻击

- 验证码的反爬原理
  - 数字类验证码：先在服务器生成一段数字，然后将数字拼接到session中，然后再通过验证码的格式显示在客户端，客户端发送数据时会将验证的输入结果放到表单中或者cookies中，由服务器进行和session对比验证
  - 所有的图片类型验证码原理类似
  - 手机短信的验证也是先在自己服务器生成验证码，然后通过第三方服务平台将码发送到客户，客户输入之后会将码携带请求一起提交。

# 第九天：9.1

## 5.5利用第三方平台做验证码验证

- 示例

  #登录简书

  [详见代码exe_1.py](C:\Users\Administrator\PycharmProjects\spider\9-1)

# 6.模拟登录

- 用户输入信息点击登录之后，浏览器就会向服务器发送一个登录请求，这个请求当中携带了用户的信息，服务器就会进行验证，验证之后会给客户端也就是浏览器一个凭证，有了凭证之后，客户端就可以去访问页面中的其他内容。

## 6.1session和cookies

- 在服务器中session记录了用户的状态（http是无状态请求，所以有了session和cookies来记录用户状态）
- cookies中只保存了session  ID的相关信息。服务器可以根据这个ID找到对应的session信息。这里面就有一个字段 ，就记录了登录状态
- cookies中也可能直接存放了一些凭证信息。



## 6.2JWT

- 近几年前后端分离越来越明显了，（前端是页面代码，不光要写CSS,JS还要写后台，一错全错）传统的前后端验证方式就是session和cookies。不适合前后端分离，所以就有了JWT(Json、Web、token)为了在网络运用传递和声明执行一种基于json的开放标准，实际上就是每次登录的时候同通过传递一个token字符串来记录登录状态。（以前需要验证session和cookies，现在只需要验证token字符串）
- JWT声明一般用来做身份提供者和服务提供者之间进行被认证的用户信息的传递，以便于客户端从服务器获取资源。也就是说，token既可以以被用来做身份验证，也可以及逆行信息的传递。
- token是一个加密过的字符串。一般会以“ . ”将token分成三段——header，payload，signature
  - header声明了JWT的签名算法，RSA、SHA256。也可能会包含JWT的编号，数据类型整个信息的base64编码
  - payload通产用来存放一些业务需要的但是又不是很敏感的信息
  - signature就是一个签名，是把前两个的信息用密钥加密后形成的，是保存在服务端，用来验证
- 三者组合起来就是用户凭证

# 第十天：9.2

模拟登陆网站：http://login2.scrape.cuiqingcai.com/

- session和cookies的核心思想：就是登录前后使用的session和cookies要保持是一对儿
- jwt，就是你要以正确的发过誓添加上这个jwt的信息就可以登录状态访问。

## 6.3JS逆向

- 对于网页来说，执行逻辑都是依靠客户端的，也就是说js代码要在客户端加载运行，特也就变成了透明的。因此导致JS的代码时不安全的。任何人都可以进行读、分析复制、甚至盗用修改
- 如果想要保证安全，就需要JS的亚索、混淆、加密技术
- 大妈亚索：取出代码中不必要的空格、换行等内容，降低代码的可读性，还可以加快代码的执行速度
- 代码混淆：使用变量替换、字符串阵列化、控制流平坦化、多态变异、将是函数、调试保护等手段。就是让你找不到变量，就是让你看不懂，就是让你找不到逻辑。
- 代码加密：可以通过一些手段代码惊醒加密，转化成人无法理解阅读的代码，到那时计算机可以执行。，抽象化加密，eval加密
- 接口加密技术
  - 数据一般通过接口来进行获取。网站或者APP可以将你请求的数据接口隐藏起来（完全开放的接口：没有任何防护措施。接口参数加密：为了提升接口的安全性，客户端和服务器约定一种接口检验方式，比如base64，hex，md5.aes des rsa 等加密手段
- JS混淆
  - 变量混淆：见含有意义的变量名、方法名、常量名随机变成无意义的乱码字符串，降低代码的可读性
  - 字符串混淆：将字符串阵列化集中放置，并可进行md5或者base64变法加密存储，使得代码中不显示明文字符串
  - 属性加密：针对JS对象属性进行加密转化，隐藏代码中的调用关系
  - 僵尸代码：无用代码，随机插入代码中。
  - 调试保护：不让代码进入debug模式，你在JS中加入一些JS的调试语句，当别人想调试你的代码时，这个语句就会执行，调试模式就不能被正常开启或者使用时不能正常跟踪。
  - 多态变异：每次看代码，代码自身每次都不一样。
  - 锁定域名：你的JSdiamagnetic只能在固定的域名中执行出正确的结果
  - 反格式化：如果你对网页的JS代码做格式化操作，然后你的网页就死掉
  - 特殊编码：自己定义规则

- JS逆行思路分析
  - 分析URL构成以及请求方式和加载数据的方式
  - URL中关键字搜索js代码文件
  - 根据url进行XHR断点调试
  - 寻找callstack中执行过的内容找到加密的手段
  - 将api/moviefan反倒一个列表中
  - 列中加入时间戳
  - 将列表内容用逗号频截
  - 将拼接好的结果用SHA1进行编码
  - 再将编码的结果和时间戳进行逗号拼接
  - 将拼接的结果以base64进行编码

# 第十一天：9.3

- 分析玩之后转化为python代码

  - 这个过程就是模拟JS的实现逻辑

  - ```python
    import time
    import base64
    from typing import List,Any
    import requests
    import hashlib
    
    index_url = "https://dynamic6.scrape.cuiqingcai.com/api/movie?limit={limit}&offset={offset}&token={token}"
    limit = 10
    offset = 0
    
    def get_token(args:List[Any]):
        timestamp = str(int(time.time()))
        args.append(timestamp)
        sign = hashlib.sha1(",".join(args).encode("utf-8")).hexdigest()
        token_ = base64.b64encode(",".join([sign, timestamp]).encode("utf-8")).decode("utf-8")
        return token_
    
    args = ["/api/movie"]
    token = get_token(args=args)
    print(token)
    url = index_url.format(limit=limit,offset=offset,token=token)
    response = requests.get(url)
    print(response.json())
    ```

# 7、超级厉害的爬虫框架——scrapy

- 比如我们用requests.aiohttp去爬取数据的时候，异常处理。数据存储，任务调度等，从头写到尾，你的代码还不一定健全
- 所以为了提高爬虫的编写效率就有了框架

## 7.1简介：

- scrapy是一个基于异步处理框架写的，scrapy框架可以完成你的各种需求，需要用python编写。只需要指定几个模块就可以进行爬取
- 框架的结构：
  - Engine（引擎）：用来处理整个系统的数据流处理、出发时间。
  - Item：定义了爬虫结果的数据结构，爬取的数据会被赋值之成该对象
  - Scheduler（调度器）：用来接收引擎发送过来的请求并且将请求加入队列，并在引擎再次发起请求的时候提供给引擎
  - downloader（下载器）：用于从网页下载内容
  - spiders（蜘蛛）：定义了爬取的逻辑和网页的解析规则，主要负责解析响应并且生成提取的结果
  - item pipline（项目管道）：负责处理从网页中抽取的项目，主要负责清晰、验证、存储数据
  - downloader Middlewares（下载器中间件）：位于引擎和下载器之间的钩子框架，主要从处理引擎和下载器之间的请求和响应
  - spider Middlewares（蜘蛛中间件）：位引擎和蜘蛛之间的钩子中间件，主要处理蜘蛛输入的响应和输出的结果以及新的的请求

scrapy框架：

![scapy框架](C:\Users\Administrator\Desktop\scapy框架.png)

老师微信：15383467476

## 7.2项目结构

- 创建命令scrapy startproject demo:demo为项目名称
- scrapy.cfg:它是scrapy项目的配置文件，其中定义了项目配置文件路经、部署相关的内容。
- iltems.py是定义数据结构的，所有的iltm的定义都可以放这里。
- piplines.py是定义item pipliine的实现。
- settings.py是全局的配置文件
- middlewares.py是定义spider middlewares和downloader Middlerwares的实现
- spiders:其中就是包含你的爬虫的url的指定和响应处理的地方

## 7.3基本运用

- 目标：创建项目、创建一个spider来抓取站点数据，然后进行处理，通过命令将抓的内容导出，将抓取的数据入库mongodb
- 创建spider的方法：
  - 进入项目然后执行
  - scrapy genspider spidername 站点域名
- 创建item
  - iltem是爬取的数据的容器，它的使用方法类似与字典，比起字典多了保护机制，可以避免拼写错误或者定义字段错误。
- 解析response
  - 之前看到了parse方法的参数为response,也就是start_urls的里面的连接的爬取结果。我们就可以直接对response变量中包含的内容进行解析。或者进一步的找去连接爬取下一个请求
  - 分析网页进行解析
- 加入iltem
  - 引入item的包进行使用
- 后续的url的请求发起
  - 构造请求时利用scrapy.Request.，这里需要传递的必要的两个参数
    ——URL和callback
  - URL是请求的连接
  - callback：是回调函数，当指定的该回调函数之后，获取的响应将会传递到这个回调函数中。
- 运行保存到文件中
  - scrapy crawl spidername -o name.json
- 使用Item pipeline
  - 将结果保存到mongo中，或者进行筛选有用的item的就可以定义item pipLine来进行实现
  - 常用的操作
    - 清洗数据
    - 验证爬取的数据，检测爬取字段
    - 查重并丢弃重复的内容
    - 将爬取的结果保存到数据库中
  - 实现也很简单就是在item pipline中定义一个process_item方法。启用这个方法之后就可以调用，需要注意的是你必须在返回结果中包含数据的字典或者iltem对象，或者抛出异常DropIltem异常
  - process_item有两参数一个item每次产生的item都会作为参数传递过来，另一个就是spider它是Spider对象。

  - open_spider 必须包含一个参数spider
  - close_spider 必须包含一个参数spider
- 在setting.py中开启item_pipline的功能

# 第十二天：9.4

# 8.Spider

- 最核心的类就是spider，定义了如何爬取某个网站的流程和相应的解析。简单来说Spider的作用有两个：1、定义爬虫的动作。2、分析爬取的网页
- 对于spider的类来说整个的爬取循环过程
  - 以初始的url初始化Request，并设置回调函数。当Request成功请求并返回后时，将产生response，作为参数传递给回调函数
  - 在回调函数中分析返回的网页。
    - 返回两种形式：1、解析后的字典类型数据和item对象。2、下一步可以经过处理或者可以直接保存的数据的连接或者内容
  - 如果返回的时item队形，就可以进行数据的存储，弱国设置了pipeline的话，会进行进一步的处理
  - 返回Request，Request执行成功后会急需的将response返回到回调函数

Sider类

- name：scrapy就是通过这个字符串初始化spider，所以在项目中它保证唯一

- allowed_domains：允许爬取的域名，不在范围内的连接不会被跟进爬取，可以不指定

- start_url：起始的url利润比送。当我们实现start_requests方法时默认会从这个列表开始抓取

- custom_settings：这是一个字典，是专属于Spider的配置，此设置会覆盖全局的设置。此设置必须在初始化前更新，所以必须定义为类变量

- crawler：此属性是由from_crawler方法设置的，代表的是spider类对应的Crawler对象，这个对象中有好多的组件，利用这个组件可以获取配置信息

- 除了以上几个方法，还有几个常用方法：

  - start_requests：此方法用于生成初始请求，并且它返回的必须是一个迭代对象，而且Soider的默认请求方法是GET，如果需要POST请求，就需要重写方法。或者改为scrapy.FormRequest()
  - parse：start_requests初次发起请求默认将响应返回的回调函数
  - close：当spider关闭时会被调用

- Selector

  - 这个选择器是基于lxml构建的，支持xpath选择器和css选择器

  - 示例

    ```python
    from scrapy import Selector
    
    body = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>hello world</title>
    </head>
    <body>
    
    </body>
    </html>
    """
    
    selector = Selector(text=body)
    title = selector.xpath("//title/text()")
    print(title)
    print(title.extract_first())
    ```

- scrapy shell
  - 其实就是帮助你分析网页
  - scrapy shell url进入shell
  - 实质上就是向这个url发送一个请求
  - 就由响应response,你就可以利用响应进行分析

- 功能强大到不行的中间件
  - Spider Middlewares
    - 这个是介入scrapy的spider处理机制的钩子架构。
    - 当downloader生成response之后，response就会被发送到Spider
      。在发送之前response就会先经过spider middlewares处理，当spider生成item时和Request之后还会经过spider Middlewares处理
    - 作用：
      - 处理downloader在给spider发送response之前，进行处理Response
      - 在Spider生成Request发送给调度器之前，进行处理。
      - Spider在item，发送给item pipeline前 进行处理
    - 使用说明：
      - 在scrapy中有定义好的SPIDER_MIDDLEWARES_BASE变量定义
      - 这个变量的内容如下：
        {
        'scrapy.spidermiddlewares.httperror.HttpErrorMiddlewares':50,
        'scrapy.spidermiddlewares.offsite.OffsiteMiddlewares':500,
        'scrapy.spidermiddlewares.referer.RerfererMiddlewares':700,
        'scrapy.spidermiddlewares.urllength.UrlLengthMiddlewares':800
        'scrapy.spidermiddlewares.depth.DepthMiddlewares':900
        }
    - 核心方法
      -  内置的基础功能
         - process_spider_input(response, spider)
           - 当response通过Spider middlewares时方法被调用，处理response
           - 返回值None或者异常。如果返回了None，scrapy将继续处理response,调用其他的spider middlewares处理该respongse .如果抛出异常。scrapy将不会再调用任何的spider middlerWares直到spider自己处理response.
             也会调用Request的errback方法。这个方法将异常重定向到中间件，将异常信息输出，调用process_spider_output，这个时候继续出异常就会让process_spider_exception处理。
         - process_spider_output(response, result, spider)
           - result:包含了Request或Item对象的可迭代对象，也是就是spider返回值
           - 返回值必须包含Request或者Item对象的可迭代对象
         - process_spider_exception(response, exception, spider)
           - exception：就是异常对象
           - 返回值:None或者一条Request或者Item。如None就会继续处理这个异常，直到所有的SPider Middlewares被调用。如果返回Request或者Item则其他的SpiderMiddlewares的process_spider_output被调用，其他的process_spider_exception就不会调用。
         - precess_start_requests(start_requests, spider)
           - start_requests:包含了Requests的可迭代对象也就是Start Requests
           - spider 也就是spider对象，就是Start Requests所属的spider
           - 其必须返回一个包含Request对象的可迭代对象

  - Downloader Middlewares
    - 它是请求和响应之间的中间件
    - 调度器出队列的请求发送下载器之前对其进行修改
    - 使用说明
      - 官方文档可查
    - 核心的方法：
      - process_requests(request, spider)
        - request：Request对像
        - spider：就是Request对像对应的spider对象
        - 调度器给下载器之前就会执行的方法。
        - 返回值必须是None或者Response对象、Request对象之一或异常。

      - process_response(requests, response, spider):
        - 下载器下载完之后，将响应给spider之前进行。
        - 返回Request对象，更低优先级的中间件执行继续放到调度器中执行，就会之再调用process_response。
        - 返回response就会调用更低级的中间件process_response，直到将所有的中间件调用完成。
        - 返回异常，会调用Request的errback方法进行回调，如果异常没有被处理最终会被系统忽略。
      - process_exception(reqeust,exception,spider)
        - 返回值None。中间件中更低级的process_exception继续执行。直到调用完毕
        - Response对象。中间件更低级process_exception就不会再执行就回去转去执行process_response。
        - Request对象。间件更低级process_exception就不会再执行就回去转去执行process_requests。

# 第十三天：9.7

# 9.Pypprteer:

基于node.js开发的一个工具，有了它就可以通过js控制你的浏览器，也可以用在爬虫是，API相当完善。就是类似与selenium,selenium的功能它全有。它其实Puppeteer的python实现版本叫Pyppeteer，在Pyppeteer中它实际上类似与Chrome执行一些动作，Chorome再发布正式的版本之前会先在Chromium上测试执行。

安装
pip install -i https://pypi.douban.com/simple pyppeteer

# 第十四天：9.8

# 10.如果遇到了Ajax页面

- 简单回顾：
  - Downloader Middlewares的用法：process_requests,process_response,process_expection
  - Item Piprline的用法：openspider,close_spider,process_item
  - item.py中定义了要抓取的数据的字段
  - spider_name：字符串是全局唯一，因为Spider这个库是根据字符串来初始化爬虫的
  - 允许爬取域名

- 动态渲染的页面请求
  - 具体使用Gerapyppeteer结合scrapy

- 分布式爬虫
  - 概念：
  - 数据结构：
    - 列表数据结构：
      lpush、lpop，rpush、rpop，可以用它实现队列的先进先出，也是先进出的栈。
      - 集合：数据的无序的且不重复，这样我么可以实现一个随机排序的不重复的爬取队列。
      - 有序集合：带有分数标识的，而scrapy的Request的方法有优先级控制。所以就可以用有序集合实现一个带有优先级队列的调度队列
- 去重：
  - hhtp://wwqwd.com?page=1
  - scrapy内部方法request_fingerprint(),根据你发起的Request,携带的method,url,body,headers来根据sha1的方法进行计算。输出一串字符，如果有一点差别就会生产新的指纹，这个指纹就是scrapy过滤重复的请求的方法
- 防止中断
  - scrapy 的request请求队列是在内存中的，只要爬虫一停这个队列就被释放了。想要做到停了还想从上次的request继续请求数据，我们在scrapy中指定一个爬虫队列的存储路径即可，scrapy crawl spider_name -s JOBDIR=crawls/spider
- 实现
  - 共享一个爬虫队列
  - 去重，
  - 需要重写调度器

- 原理
  - 将Request对象放到数据库中，但是数据库是存不了对象的，所有要将对象转换成字符串在存储，序列化，反序列化的操作利用pickle的库实现的，一般用push将Request对象存储到数据库中会调用_encode_request进行序列化，然后用pop取出数据，然后利用_decode_request方法进行反序列化

# 第十五天：9.9